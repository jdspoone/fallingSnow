\documentclass[conference]{acmsiggraph}

\title{Real-time Rendering of Falling Snow}

\author{Brent Glowinski, Jeff Spooner, and Parker Fish}
\pdfauthor{The above}

\keywords{snow, real-time, fog, particles}

\begin{document}

\maketitle

\begin{abstract}
This report describes the results of our project to render falling snow in real-time. The infrastructure for the particle model was implemented on the GPU via OpenGL's \textit{transform feedback} capabilities. Snowflake geometry was created through the use of a geometry shader, with variable translucency implemented in the fragment shader. Wind was simulated with a 3D texture, mapping particle positions to random velocities, and basic snow aggregation was implemented by blending the background scene with the colour of snow based upon snow density and distance from the camera.

\end{abstract}

\keywordlist

\copyrightspace

\section{Introduction and Motivation}
Snow is a common weather event in cold climates, both in reality and in various media. A snowy scene can set the mood effectively in any number of situations and provide a more authentic feel to a scene when implemented well. However the requirements of real-time rendering make implementing a good solution difficult, and in comparison to other natural phenomena such as fire, the rendering of snow has not been explored thoroughly.

Sims et al. \cite{sims90} demonstrated that previous work on particle systems could be applied to snow specifically. Govindaraju et al. \cite{govindaraju05} implemented GPU sorting which can be used to sort snowflakes by distance from the camera. While this approach would afford us a number of interesting possibilities, it very difficult to implement, as well as being much slower than we would like. Moeslund et al. \cite{moeslund05} demonstrated how to realistically model the structure of individual snowflakes, as well as the movement of snowflakes as they fall. While randomly generated snowflakes could potentially look very good if done correctly, the limitations of the geometry shader, as well as our general concern for efficiency, has ruled this out. Additionally, while the full fluid model provides the best simulation of real wind, we do not feel it is feasible for a real-time application. Deokar \cite{deokar09} implements all the above in addition to reduced visibility due to aggregation of snow. 

Our goal is to provide a real-time rendering of falling snow that both looks reasonably realistic and runs well on a variety of recent hardware. We accomplish this by implementing a particle system that runs primarily on the GPU, allowing large systems to be simulated quickly due to heavy parallelism. Our system has three primary components to affect authenticity: an effective wind model that allows snow to move in a believable fashion; snowflake models that can be rendered quickly but still look nice enough to fool the brain when in motion; and a simple distance-based fog to facilitate the appearance of high-density snowfall.

\section{Methodology}
Fog aggregation is calculated in a manner that assumes the snow volume is constant across the scene. While this is not particularly accurate - especially when particles are first created and are primarily at the top of the scene - it does allow for a more simplified implementation and makes modifying the formula for a good result much simpler. We determine the effect of the fog on visibility to be:
\begin{equation} \label{eq:fog}
\frac{p_{cur}}{p_{const}}d^2
\end{equation}
where $p_{cur}$ is the number of particles in the scene, $p_{const}$ is a constant which we set to five million (so that when five million particles are in the scene the term evaluates to 1) and $d$ is the distance to the point. This is similar to the visibility equation presented in Deokar's thesis \cite{deokar09}, however we use it as the factor of a linear interpolation between the background colour and an object's normal colour.

The background colour changes based on the number of particles in the scene to approximate the effect of "blowing out" the image due to heavy fog. This is a common effect when there are lots of lights in foggy conditions, and since our scene is not dark it seemed appropriate to implement this. We determined the value using a simple heuristic:
\begin{equation} \label{eq:bgcolour}
colour = c\sqrt[5]{p_{cur}}
\end{equation}
Where $c$ is a constant determined to be 0.04 by setting the colour to 0.5, an appropriate midpoint, and setting $p_{cur}$ to 300000 as our starting value. $\sqrt[5]{}$ was chosen since it scaled fairly well to higher values.

Each particle has a radial fade-out applied to it in order to give the snowflake a softer appearance. This was simply determined to provide a nice looking result that softened the appearance of snowflakes while maintaining their visibility.
\begin{equation} \label{eq:alpha}
\frac{0.3 \cdot (0.0016 - distance^{0.9})} {0.001}
\end{equation}
In this equation, $distance$ is simply the distance from the centre of the snowflake to the pixel being drawn.

\section{Implementation}

\subsection{Particle Model}
For a simulation of falling snow to approach realism, individual snowflakes must have independent and variable velocities. While it is possible to model this entirely on the CPU, this approach quickly becomes infeasible as the number of snowflakes increases. This, along with the fact that the position and velocity attributes of a given particle are independent of those of all others, makes the GPU the ideal place to implement a particle model.

The particle model used in this project was implemented by way of OpenGL's \textit{transform feedback} functionality, which allows for general purpose computation to be outsourced to the GPU. During \textit{transform feedback}, a shader program will write one or more of its output variables into a vertex buffer object (henceforth referred to as a \textit{VBO}). Our implementation included a dedicated transform feedback shader program, with the sole purpose of updating the positions, velocities and rotation angles of snowflakes, 

After compiling the necessary shader files, but before linking the program, the feedback varyings must be specified using the \textit{glTransformFeedbackVaryings} method. This is where we specify, as an array of strings, the labels of the output variables we wish to write to our output \textit{VBOs}. In the case of this project, they are \textit{nextPosition, nextVelocity} and \textit{nextAngle}. Additionally, we must specify whether we wish the output to be written to separate \textit{VBOs}, or interleaved on a single \textit{VBO}, using \textit{GL\_SEPARATE\_ATTRIBS} or \textit{GL\_INTERLEAVED\_ATTRIBS} respectively. After this, linking the program occurs.

For each of the vertex attributes we wish to continually update on the GPU, there are two \textit{VBOs}, specifically \textit{positionVBO[2]}, \textit{velocityVBO[2]} and \textit{angleVBO[2]}. Each time \textit{transform feedback} is performed, the input vertex attributes will be bound from the \textit{VBOs} at index \textit{iteration \% 2}, where \textit{iteration} is the number of times the rendering loop has run. Then, the feedback varyings will be written out to the \textit{VBOs} at index \textit{(iteration + 1) \% 2}. The output buffers are specified using the \textit{glBindBufferBase} method. Finally, after \textit{transform feedback} is performed, the pointers to various \textit{VBOs} are swapped, for instance \textit{swap(positionVBO[0], positionVBO[1])}. The shader program responsible for rendering snowflakes to screen will always bind its input vertex attributes from the \textit{VBOs} at index \textit{iteration \% 2}.

The shader program required for \textit{transform feedback} was composed of only one programmable portion, the vertex shader. Within this shader program we update the particle velocities, positions, and rotation angles, the specifics of which will be described in the following sections. Initially we had attempted to combine \textit{transform feedback} with the shader program responsible for rendering snowflakes to screen, but this proved to be something of a problem, so we opted to maintain two separate programs. During the main rendering loop, we first render the snowflakes to screen, and then perform \textit{transform feedback} to update the snowflake's various attributes in a separate pass.

Upon launching our snow rendering program, snowflakes are distributed randomly throughout the snow box, which has length and width $2.0$ and a height of $1.0$. All particles are initialized with a constant downward velocity and random rotation angle. These attributes are stored in C++ vectors and then loaded into dedicated \textit{VBOs}. As the program runs and snowflakes move, they are looped within the snow field during \textit{transform feedback}. For example, if a snowflake reaches the bottom of the field, it will be flipped to the top. Our particle model allows for snowflakes to be added to and removed from the scene dynamically. When we wish to spawn more snowflakes, we create random points along the top face of the snow box, along with the standard initial downward velocity and random rotation angle, and add them to the scene over the course of a few seconds. This allows for snow density to be increased in a gradual and more realistic fashion than immediately adding the snowflakes throughout the scene would. Removing particles relies on the fact that snowflakes always created and stored randomly. We merely remove the last $n$ snowflake attributes from the various \textit{VBOs}, and in all likelihood the snow density will be reduced uniformly. While this is certainly not an infallible approach, we have yet to experience any significant problems with it.

\subsection{Snowflake Geometry}
The shader program used to render snowflakes to screen takes advantage of both a custom geometry and fragment shader. Particle positions are passed into the shader program as vertex attributes, which are transformed into camera space in the vertex shader. Then, the actual snowflake is built around that point in the geometry shader. As of our final presentation, the geometry in question was a level one Koch snowflake, simplified to be two equilateral triangles facing opposite directions and centred at the given particle position. For the sake of efficiency, and because trigonometric functions on the GPU are very expensive, we calculated the vertices of the unit equilateral triangle, namely the triangle which circumscribes the unit circle, on the CPU and then passed them to our shader program as uniforms. Emitting the six-pointed star becomes a simple matter of scaling the uniforms by the radius of the snowflake, applying the rotation matrix corresponding to the given rotation angle (passed into the shader program as a vertex attribute), and then emitting three vertices using the scaled and rotated vertices as offsets to the given snowflake position.

Initially we had hoped to implement more interesting snowflake geometry, specifically full Koch snowflakes, but the limitations of the geometry shader prevented us from doing so. Firstly, the value to which one sets a geometry shaders \textit{max\_vertices} is number of vertices that will always be emitted, regardless of how many are explicitly emitted. This means that if we emit snowflakes of varying complexity in the same geometry shader based on some criteria, distance from the camera for example, there would be no difference in terms of efficiency between the most complicated snowflake and a single triangle. Additionally, even if this were not a concern, the branching which would be required to select the correct snowflake to emit would become a bottleneck of its own. Branching on the GPU is only efficient if each branch is used roughly half of the time. Since the criteria we would be branching on would be distance from the camera, and since most particles will be relatively far away from the camera, branching would be very inefficient.

The snowflake translucency takes advantage of \textit{alpha blending} within OpenGL. This feature is enabled by calling \textit{glBlendFunc}, specifying the source fragment alpha the destination fragment field. The source is what you are currently trying to render, and the destination is what is currently in the framebuffer. The parameters used in our project were \textit{GL\_SRC\_ALPHA}  and \textit{GL\_ONE} respectively.

This blending complemented the geometry with which we built our snowflakes. The alpha of the source fragment was calculated in the fragment shader using equation~\ref{eq:alpha} by passing the midpoint of the snowflake through the geometry shader. The closer a fragment is to the closer to the centre of the snowflake, the more opaque it will be, and the further away it is the more transparent, bottoming out at 0.

\subsection{Perlin Noise Wind}
While a full fluid dynamics simulation is required for the most accurate results in modelling the movement of snowflakes, it is far too slow to use for an extremely large system. Instead, we generate a 3D texture before simulation starts to determine each particle's velocity as it moves through the scene.

We start by defining a 3D array of size 256 in each dimension. This is the lowest resolution texture we could generate that still gave us reasonably varied movement with our snowflakes; lower resolutions tended to result in more predictable but less interesting movements. Each element of this array is initialized to three randomized float values, \textit{x}, \textit{y}, and \textit{z}. These values are generated using the standard \textit{rand()} function in the range of [0..1] then adjusted as desired: \textit{x} is made negative to go to the left, \textit{y} is halved to make snowflakes fall slower, and \textit{z} has 0.5 subtracted from it to put it in the range of [-0.5..0.5], thus allowing the particle to move back and forth in the z-axis.

Turbulence is then applied to the texture by taking several "zoomed in" versions of the random array, smoothing over them using a bilinear filter, and adding them together. To elaborate, we iterate over the array and, for each cell, we take three zoom-levels of the texture by simply dividing the index by a factor of two at each level. Since this returns a floating point value, we interpolate between the nearest integer coordinates to determine the value of the array at that point. Adding these interpolated values together, with the more zoomed-in values given more weight, gives us the final texture. This texture is stored in a separate array since storing them back into the original array results in all the values blending together due to the filtering.

The final 3D texture is passed into the \textit{transform feedback} shader. Each particle uses its position - normalized to the range of [0..1] -  as an index to get a velocity. This velocity is then mixed with a downward velocity by an amount based on the particle's index. This causes some particles to follow the texture more closely than others, providing more variance between their movements and less banding when they follow similar paths. The next position written to the output \textit{VBO} is simply the previous position minus this mixed velocity. Initially we intended to write the mixed velocity to our output \textit{VBO} to use as the previous velocity in the next iteration of \textit{transform feedback}, but this led to excessive banding. Maintaining the initial downward velocity as the previous velocity throughout the program mitigated this problem substantially.

\subsection{Fog Aggregation}
Fog aggregation is determined based on the number of particles in the scene as well as the distance from the camera. The assumptions mentioned involving equation~\ref{eq:fog} make it so that the implementation requires little calculation and is done entirely in a fragment shader applied to the trees and floor. The colour of those objects is determined by mixing the background colour with the object's normal colour by the value calculated by equation~\ref{eq:fog}.

When we add or remove particles, we recalculate the background colour using equation~\ref{eq:bgcolor}. We additionally use max/min functions to keep this value between 0.3 and 0.9 in order to keep it from going totally black or totally white as both results were too extreme to be reasonable.

\section{Results}
The video included in this submission demonstrates the results of our project. Initially we show the falling snow without the scene being rendered as well, and demonstrated the variable wind speed. Then we demonstrate rendering the scene and the falling snow together. During this, snow aggregation is shown as well, first with a low density of snow, then again after increasing the snow density significantly. Lastly we render the scene alone for the sake of contrast. 

The source code for our project can be found at: 

\url{https://github.com/jdspoone/fallingSnow}

\section{Conclusion}
In summary, the significant portions of our real-time snow renderer were the following: a particle model implemented on the GPU via \textit{transform feedback}; individual snowflakes being emitted from a geometry shader; \textit{perlin noise} wind via a 3D texture mapping particle positions to velocities; and a simple approximation of snow aggregation.

Prior to this project we had not done any general purpose computation on the GPU, and implementing a particle model in this fashion was very informative. In particular, the lack of non-trivial tutorials on the use of \textit{transform feedback} forced us to closely examine and ultimately understand the OpenGL pipeline much better than we did previously. Working with the geometry shader was also very interesting, both in terms of the interesting possibilities and limitations arising from its use. Because the real-time nature of the project, we were forced to take efficiency into consideration more so than during the this course's assignments, particularly when it came to our various shader programs.

Future work on this project could proceed in a number of different directions. Firstly, an attempt at particle sorting on the GPU could be made, which if successful would allow us do a number of interesting things. For example, intelligently adding and deleting particles, and avoiding the efficiency problems of the geometry shader by rendering snowflakes with different shader programs depending on their distance from the camera. This would however, most likely require the use of another method for general purpose computation on the GPU, as we suspect \textit{transform feedback} is not at all suited for sorting. Additionally or alternatively, the limitations of the geometry shader could potentially be mitigated by introducing a tessellation shader to the snowflake shader program, which would be responsible for the level of detail of individual snowflakes. A more sophisticated solution than particle looping to the particle lifecycle problem could be explored as well. Snow aggregation could be made more complex than our current, straightforward implementation of it, potentially taking wind field into account.

\bibliographystyle{acmsiggraph}
\nocite{*}
\bibliography{docbib}

\end{document}
