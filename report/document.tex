\documentclass[conference]{acmsiggraph}

\title{Real-time Rendering of Falling Snow}

\author{Brent Glowinski, Jeff Spooner, and Parker Fish}
\pdfauthor{The above}

\keywords{snow, real-time, fog, particles}

\begin{document}

\maketitle

\begin{abstract}

Abstract goes here.

\end{abstract}

\keywordlist

\copyrightspace

\section{Introduction and Motivation}
Blah blah blah boring things.

\section{Methodology}
Fog aggregation is calculated in a manner that assumes the snow volume is constant across the scene. While this is not particularly accurate - especially when particles are first created and are primarily at the top of the scene - it does allow for a more simplified implementation and makes modifying the formula for a good result much simpler. We determine the effect of the fog on visibility to be:
\begin{equation} \label{eq:fog}
\frac{p_{cur}}{p_{const}}d^2
\end{equation}
where $p_{cur}$ is the number of particles in the scene, $p_{const}$ is a constant which we set to five million (so that when five million particles are in the scene the term evaluates to 1) and $d$ is the distance to the point. This is similar to the visibility equation presented in Deokar's thesis \cite{deokar09}, however we use it as the factor of a linear interpolation between the background colour and an object's normal colour.

The background color changes based on the number of particles in the scene to approximate the effect of "blowing out" the image due to heavy fog. This is a common effect when there are lots of lights in foggy conditions, and since our scene is not dark it seemed appropriate to implement this. We determined the value using a simple heuristic:
\begin{equation} \label{eq:bgcolor}
colour = c\sqrt[5]{p_cur}
\end{equation}
Where $c$ is a constant determined to be 0.04 by setting the colour to 0.5, an appropriate midpoint, and setting $p_{cur}$ to 300000 as our starting value. $\sqrt[5]{}$ was chosen since it scaled fairly well to higher values.

\section{Implementation}

\subsection{Particle Model}
For a simulation of falling snow to approach realism, individual snowflakes must have independent and variable velocities. While it is possible to model this entirely on the CPU, this approach quickly becomes infeasible as the number of snowflakes increases. This, along with the fact that the position and velocity attributes of a given particle are independent of those of all others, makes the GPU the ideal place to implement a particle model.

The particle model used in this project was implemented by way of OpenGL's \textit{transform feedback} functionality, which allows for general purpose computation to be outsourced to the GPU. During \textit{transform feedback}, a shader program will write one or more of its output variables into a vertex buffer object (henceforth referred to as a \textit{VBO}). Our implementation included a dedicated transform feedback shader program, with the sole purpose of updating the positions, velocities and rotation angles of snowflakes, 

After compiling the necessary shader files, but before linking the program the feedback varyings must be specified using the \textit{glTransformFeedbackVaryings} method. This is where we specify, as an array of strings, the labels of the output variables we wish to write to \textit{VBOs}. In the case of this project, they are \textit{nextPosition, nextVelocity} and \textit{nextAngle}. Additionally, we must specify whether we wish the output to be written to separate \textit{VBOs}, or interleaved on a single \textit{VBO}, using \textit{GL\_SEPARATE\_ATTRIBS} or \textit{GL\_INTERLEAVED\_ATTRIBS} respectively. After this, linking the program occurs.

For each of the vertex attributes we wish to continually update on the GPU, there are two \textit{VBOs}, specifically \textit{positionVBO[2]}, \textit{velocityVBO[2]} and \textit{angleVBO[2]}. Each time \textit{transform feedback} is performed, the input vertex attributes will be bound from the \textit{VBOs} at index \textit{iteration \% 2}, where \textit{iteration} is the number of times the rendering loop has run. Then, the feedback varyings will be written out to the \textit{VBOs} at index \textit{(iteration + 1) \% 2}. The output buffers are specified using the \textit{glBindBufferBase} method. Finally, after \textit{transform feedback} is performed, the pointers to various \textit{VBOs} are swapped, for instance \textit{swap(positionVBO[0], positionVBO[1])}.

\subsection{Snowflake Geometry}
The shader program used to render snowflakes to screen took advantage of both a custom geometry and fragment shader. Particle positions are passed into the shader program as vertex attributes, which are transformed into camera space in the vertex shader. Then, the actual snowflake is built around that point in the geometry shader. As of our presentation, the geometry in question was a level one Koch snowflake, simplified to be two equilateral triangles facing opposite directions and centred at the given particle position. For the sake of efficiency, and because trigonometric functions on the GPU are very expensive, we calculated the vertices of the unit equilateral triangle, namely the triangle which circumscribes the unit circle, on the CPU and then passed them to our shader program as uniforms. Emitting the six-pointed star becomes a simple matter of scaling the uniforms by the radius of the snowflake, applying the rotation matrix corresponding to the given rotation angle, and then emitting three vertices using the scaled and rotated vertices as offsets to the given snowflake position.

Initially we had hoped to implement more interesting snowflake geometry, specifically full koch snowflakes, but the limitations of the geometry shader prevented us from doing so. Firstly, the value to which one sets a geometry shaders \textit{max\_vertices} is number of vertices that will always be emitted, regardless of how many are explicitly set. This means that if we emitted snowflakes of varying complexity in the same geometry shader based on some criteria, distance from the camera for example, there would be no difference in terms of efficiency between the most complicated snowflake and a single triangle. Additionally, even if this were not a concern, the branching which would be required to select the correct snowflake to emit given the circumstances would become a bottleneck of its own. Branching on the GPU is only efficient if each branch is used roughly half of the time. Since the criteria we would be branching on would be distance from the camera, and since most particles will be relatively far away from the camera, branching would be very inefficient.

Snowflake translucency...

\subsection{Perlin Wind}
While a full fluid dynamics simulation is required for the most accurate results in modelling the movement of snowflakes, it is far too slow to use for an extremely large system. Instead, a 3D texture is generated before simulation starts to determine each particle's velocity as it moves through the scene.

We start by defining a 3D array of size 256 in each dimension. This is the lowest resolution texture we could generate that still gave us reasonably varied movement with our snowflakes; lower resolutions tended to result in more predictable but less interesting movement of snowflakes. Each element of this array is initialized to three randomized float values, \textit{x}, \textit{y}, and \textit{z}. These values are generated using the standard \textit{rand()} function in the range of [0..1] then adjusted as desired: \textit{x} is made negative to go to the left, \textit{y} is halved to make snowflakes fall slower, and \textit{z} has 0.5 subtracted from it to put it in the range of [-0.5..0.5], thus allowing it to move back and forth.

Turbulence is then applied to the texture by taking several "zoomed in" versions of the random array, smoothing over them using a bilinear filter, and adding them together. To elaborate, we iterate over the array and, for each cell, we take three zoom-levels of the texture by simply dividing the index by a factor of two at each level. Since this returns a floating point value, we interpolate between the nearest integer coordinates to determine the value of the array at that point. Adding these interpolated values together, with the more zoomed-in values given more weight, gives us the final texture. This texture is stored in a separate array since storing them back into the original array results in all the values blending together due to the filtering.

The final 3D texture is passed into the transform feedback shader. Each particle uses its position - normalized to the range of [0..1] -  as an index to get a velocity. This velocity is then mixed with a downward velocity by an amount based on the particle's index. This causes some particles to follow the texture more closely than others, providing more variance between their movements and less banding when they follow the same path. To get the next position, we simply subtract this mixed velocity from the previous position.

\subsection{Fog Aggregation}
Fog aggregation is determined based on the number of particles in the scene as well as the distance from the camera. The assumptions mentioned involving equation~\ref{eq:fog} make it so that the implementation requires little calculation and is done entirely in a fragment shader applied to the trees and floor. The colour of those objects is determined by mixing the background colour with the object's normal colour by a value calculated by equation~\ref{eq:fog}.

When we add or remove particles, we recalculate the background colour using equation~\ref{eq:bgcolor}. We additionally use max/min functions to keep this value between 0.3 and 0.9 in order to keep it from going totally black or totally white as both results looked terrible.

\section{Results}
Look cool.

\section{Conclusion}
In summary, the significant portions of our real-time snow renderer were the following: a particle model implemented on the GPU via \textit{transform feedback}; individual snowflakes being emitted from a geometry shader; perlin noise wind via a 3D texture mapping particle positions to velocities; and a simple approximation of snow aggregation.

Prior to this project we had not done any general purpose computation on the GPU, and implementing a particle model in this fashion was very informative. In particular, the lack of non-trivial tutorials on the use of \textit{transform feedback} forced us to closely examine and ultimately understand the OpenGL pipeline much better than we did previously. Working with the geometry shader was also very interesting, both in terms of the interesting possibilities and limitations arising from its use. Because the real-time nature of the project, we were forced to take efficiency into consideration more so than during the this course's assignments, particularly when it came to our various shader programs.

Future work on this project could proceed in a number of different directions. Firstly, an attempt at particle sorting on the GPU could be made, which if successful would allow us do a number of interesting things. For example, intelligently adding and deleting particles, and avoiding the efficiency problems of the geometry shader by rendering snowflakes with different shader programs depending on their distance from the camera. This would however, most likely require the use of another method for general purpose computation on the GPU, as we suspect \textit{transform feedback} is not at all suited for sorting. Additionally or alternatively, the limitations of the geometry shader could be potentially mitigated by introducing a tessellation shader responsible for the level of detail of individual snowflakes. A more sophisticated solution than particle looping to the particle lifecycle problem could be explored as well. Snow aggregation could be made more complex than our current, straightforward implementation of it, potentially taking wind field into account.

\bibliographystyle{acmsiggraph}
\nocite{*}
\bibliography{docbib}

\end{document}
