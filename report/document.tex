\documentclass[conference]{acmsiggraph}

\title{Real-time Rendering of Falling Snow}

\author{Brent Glowinski, Jeff Spooner, and Parker Fish}
\pdfauthor{The above}

\keywords{snow, real-time, fog, particles}

\begin{document}

\maketitle

\begin{abstract}

Abstract goes here.

\end{abstract}

\keywordlist

\copyrightspace

\section{Introduction and Motivation}
Blah blah blah boring things.

\section{Methodology}
Slightly more interesting things? Formulae go here.

\section{Implementation}

\subsection{Particle Model}
For a simulation of falling snow to approach realism, individual snowflakes must have independent and variable velocities. While it is possible to model this entirely on the CPU, this approach quickly becomes infeasible as the number of snowflakes increases. This, along with the fact that the position and velocity attributes of a given particle are independent of those of all others, makes the GPU the ideal place to implement a particle model.

The particle model used in this project was implemented by way of OpenGL's \textit{transform feedback} functionality, which allows for general purpose computation to be outsourced to the GPU. During \textit{transform feedback}, a shader program will write one or more of its output variables into a vertex buffer object (henceforth referred to as a \textit{VBO}). Our implementation included a dedicated transform feedback shader program, with the sole purpose of updating the positions, velocities and rotation angles of snowflakes, 

After compiling the necessary shader files, but before linking the program the feedback varyings must be specified using the \textit{glTransformFeedbackVaryings} method. This is where we specify, as an array of strings, the labels of the output variables we wish to write to \textit{VBOs}. In the case of this project, they are \textit{nextPosition, nextVelocity} and \textit{nextAngle}. Additionally, we must specify whether we wish the output to be written to separate \textit{VBOs}, or interleaved on a single \textit{VBO}, using \textit{GL\_SEPARATE\_ATTRIBS} or \textit{GL\_INTERLEAVED\_ATTRIBS} respectively. After this, linking the program occurs.

For each of the vertex attributes we wish to continually update on the GPU, there are two \textit{VBOs}, specifically \textit{positionVBO[2]}, \textit{velocityVBO[2]} and \textit{angleVBO[2]}. Each time \textit{transform feedback} is performed, the input vertex attributes will be bound from the \textit{VBOs} at index \textit{iteration \% 2}, where \textit{iteration} is the number of times the rendering loop has run. Then, the feedback varyings will be written out to the \textit{VBOs} at index \textit{(iteration + 1) \% 2}. The output buffers are specified using the \textit{glBindBufferBase} method. Finally, after \textit{transform feedback} is performed, the pointers to various \textit{VBOs} are swapped, for instance \textit{swap(positionVBO[0], positionVBO[1])}.

\subsection{Snowflake Geometry}
The shader program used to render snowflakes to screen took advantage of both a custom geometry and fragment shader. Particle positions are passed into the shader program as vertex attributes, which are transformed into camera space in the vertex shader. Then, the actual snowflake is built around that point in the geometry shader. As of our presentation, the geometry in question was a level one Koch snowflake, simplified to be two equilateral triangles facing opposite directions and centred at the given particle position. For the sake of efficiency, and because trigonometric functions on the GPU are very expensive, we calculated the vertices of the unit equilateral triangle, namely the triangle which circumscribes the unit circle, on the CPU and then passed them to our shader program as uniforms. Emitting the six-pointed star becomes a simple matter of scaling the uniforms by the radius of the snowflake, applying the rotation matrix corresponding to the given rotation angle, and then emitting three vertices using the scaled and rotated vertices as offsets to the given snowflake position.

Initially we had hoped to implement more interesting snowflake geometry, specifically full koch snowflakes, but the limitations of the geometry shader prevented us from doing so. Firstly, the value to which one sets a geometry shaders \textit{max\_vertices} is number of vertices that will always be emitted, regardless of how many are explicitly set. This means that if we emitted snowflakes of varying complexity in the same geometry shader based on some criteria, distance from the camera for example, there would be no difference in terms of efficiency between the most complicated snowflake and a single triangle. Additionally, even if this were not a concern, the branching which would be required to select the correct snowflake to emit given the circumstances would become a bottleneck of its own. Branching on the GPU is only efficient if each branch is used roughly half of the time. Since the criteria we would be branching on would be distance from the camera, and since most particles will be relatively far away from the camera, branching would be very inefficient.

Snowflake translucency...

\subsection{Perlin Wind}
While a full fluid dynamics simulation is required for the most accurate results in modelling the movement of snowflakes, it is far too slow to use for an extremely large system. Instead, a 3D texture is generated before simulation starts to determine each particle's velocity as it moves through the scene.

We start by defining a 3D array of size 256 in each dimension. This is the lowest resolution texture we could generate that still gave us reasonably varied movement with our snowflakes; lower resolutions tended to result in more predictable but less interesting movement of snowflakes. Each element of this array is initialized to three randomized float values, \textit{x}, \textit{y}, and \textit{z}. These values are generated using the standard \textit{rand()} function in the range of [0..1] then adjusted as desired: \textit{x} is made negative to go to the left, \textit{y} is halved to make snowflakes fall slower, and \textit{z} has 0.5 subtracted from it to put it in the range of [-0.5..0.5], thus allowing it to move back and forth.

Turbulence is then applied to the texture by taking several ``zoomed in'' versions of the random array, smoothing over them using a bilinear filter, and adding them together. To elaborate, we iterate over the array and, for each cell, we take three zoom-levels of the texture by simply dividing the index by a factor of two at each level. Since this returns a floating point value, we interpolate between the nearest integer coordinates to determine the value of the array at that point. Adding these interpolated values together, with the more zoomed-in values given more weight, gives us the final texture. This texture is stored in a separate array since storing them back into the original array results in all the values blending together due to the filtering.

The final 3D texture is passed into the transform feedback shader. Each particle uses its position - normalized to the range of [0..1] -  as an index to get a velocity. This velocity is then mixed with a downward velocity by an amount based on the particle's index. This causes some particles to follow the texture more closely than others, providing more variance between their movements and less banding when they follow the same path. To get the next position, we simply subtract this mixed velocity from the previous position.

\subsection{Fog Aggregation}
Fog aggregation is determined based on the number of particles in the scene as well as the distance from the camera. We assume the particles are uniformly distributed throughout the scene, although this is untrue when they are first added, so the implementation requires little calculation and is done entirely in a fragment shader applied to the trees and floor. The colour of those objects is determined by mixing the background colour with the object's normal colour by a value calculated by $\frac{numParticles}{5000000}distance^2$. Five million was simply determined to be a result that provided a good initial look for average snowfall and scaled nicely as particles were added and removed. 

To accentuate the effect, we also change the background colour based on the number of particles. This helps to attain the blown-out effect that can be seen when looking through very heavy fog in the snow, especially when there are other light sources being diluted through the fog. When we add or remove particles, we recalculate the background colour to be $0.04 \sqrt[5]{numParticles}$, again chosen based on a nice looking initial value with good scaling.

\section{Results}
Look cool.

\section{Conclusion}
Finished! Huzzah!

\bibliographystyle{acmsiggraph}
\nocite{*}
\bibliography{template}
\end{document}
